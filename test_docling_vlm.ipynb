{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b81d14-2834-4bf6-8af7-aabd0589072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b1c7a-ed39-4927-8f0b-c2707dabd2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "from docling_core.types.doc.page import SegmentedPage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    VlmPipelineOptions,\n",
    ")\n",
    "from docling.datamodel.pipeline_options_vlm_model import ApiVlmOptions, ResponseFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "### Example of ApiVlmOptions definitions\n",
    "\n",
    "#### Using LM Studio or VLLM (OpenAI-compatible APIs)\n",
    "\n",
    "\n",
    "def openai_compatible_vlm_options(\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    format: ResponseFormat,\n",
    "    hostname_and_port,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 4096,\n",
    "    api_key: str = \"\",\n",
    "    skip_special_tokens=False,\n",
    "):\n",
    "    headers = {}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "        headers[\"Content-Type\"] = \"application/json\"\n",
    "\n",
    "    options = ApiVlmOptions(\n",
    "        url=f\"http://{hostname_and_port}/v1/chat/completions\",  # LM studio defaults to port 1234, VLLM to 8000\n",
    "        params=dict(\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            skip_special_tokens=skip_special_tokens,  # needed for VLLM\n",
    "        ),\n",
    "        headers=headers,\n",
    "        prompt=prompt,\n",
    "        timeout=90,\n",
    "        scale=2.0,\n",
    "        temperature=temperature,\n",
    "        response_format=format,\n",
    "    )\n",
    "    return options\n",
    "\n",
    "\n",
    "#### Using LM Studio with OlmOcr model\n",
    "\n",
    "\n",
    "def lms_olmocr_vlm_options(model: str):\n",
    "    class OlmocrVlmOptions(ApiVlmOptions):\n",
    "        def build_prompt(self, page: Optional[SegmentedPage]) -> str:\n",
    "            if page is None:\n",
    "                return self.prompt.replace(\"#RAW_TEXT#\", \"\")\n",
    "\n",
    "            anchor = [\n",
    "                f\"Page dimensions: {int(page.dimension.width)}x{int(page.dimension.height)}\"\n",
    "            ]\n",
    "\n",
    "            for text_cell in page.textline_cells:\n",
    "                if not text_cell.text.strip():\n",
    "                    continue\n",
    "                bbox = text_cell.rect.to_bounding_box().to_bottom_left_origin(\n",
    "                    page.dimension.height\n",
    "                )\n",
    "                anchor.append(f\"[{int(bbox.l)}x{int(bbox.b)}] {text_cell.text}\")\n",
    "\n",
    "            for image_cell in page.bitmap_resources:\n",
    "                bbox = image_cell.rect.to_bounding_box().to_bottom_left_origin(\n",
    "                    page.dimension.height\n",
    "                )\n",
    "                anchor.append(\n",
    "                    f\"[Image {int(bbox.l)}x{int(bbox.b)} to {int(bbox.r)}x{int(bbox.t)}]\"\n",
    "                )\n",
    "\n",
    "            if len(anchor) == 1:\n",
    "                anchor.append(\n",
    "                    f\"[Image 0x0 to {int(page.dimension.width)}x{int(page.dimension.height)}]\"\n",
    "                )\n",
    "\n",
    "            # Original prompt uses cells sorting. We are skipping it for simplicity.\n",
    "\n",
    "            raw_text = \"\\n\".join(anchor)\n",
    "\n",
    "            return self.prompt.replace(\"#RAW_TEXT#\", raw_text)\n",
    "\n",
    "        def decode_response(self, text: str) -> str:\n",
    "            # OlmOcr trained to generate json response with language, rotation and other info\n",
    "            try:\n",
    "                generated_json = json.loads(text)\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                return \"\"\n",
    "\n",
    "            return generated_json[\"natural_text\"]\n",
    "\n",
    "    options = OlmocrVlmOptions(\n",
    "        url=\"http://localhost:1234/v1/chat/completions\",\n",
    "        params=dict(\n",
    "            model=model,\n",
    "        ),\n",
    "        prompt=(\n",
    "            \"Below is the image of one page of a document, as well as some raw textual\"\n",
    "            \" content that was previously extracted for it. Just return the plain text\"\n",
    "            \" representation of this document as if you were reading it naturally.\\n\"\n",
    "            \"Do not hallucinate.\\n\"\n",
    "            \"RAW_TEXT_START\\n#RAW_TEXT#\\nRAW_TEXT_END\"\n",
    "        ),\n",
    "        timeout=90,\n",
    "        scale=1.0,\n",
    "        max_size=1024,  # from OlmOcr pipeline\n",
    "        response_format=ResponseFormat.MARKDOWN,\n",
    "    )\n",
    "    return options\n",
    "\n",
    "\n",
    "#### Using Ollama\n",
    "\n",
    "\n",
    "def ollama_vlm_options(model: str, prompt: str):\n",
    "    options = ApiVlmOptions(\n",
    "        url=\"http://localhost:11434/v1/chat/completions\",  # the default Ollama endpoint\n",
    "        params=dict(\n",
    "            model=model,\n",
    "        ),\n",
    "        prompt=prompt,\n",
    "        timeout=90,\n",
    "        scale=1.0,\n",
    "        response_format=ResponseFormat.MARKDOWN,\n",
    "    )\n",
    "    return options\n",
    "\n",
    "\n",
    "#### Using a cloud service like IBM watsonx.ai\n",
    "\n",
    "\n",
    "def watsonx_vlm_options(model: str, prompt: str):\n",
    "    load_dotenv()\n",
    "    api_key = os.environ.get(\"WX_API_KEY\")\n",
    "    project_id = os.environ.get(\"WX_PROJECT_ID\")\n",
    "\n",
    "    def _get_iam_access_token(api_key: str) -> str:\n",
    "        res = requests.post(\n",
    "            url=\"https://iam.cloud.ibm.com/identity/token\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            },\n",
    "            data=f\"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={api_key}\",\n",
    "        )\n",
    "        res.raise_for_status()\n",
    "        api_out = res.json()\n",
    "        print(f\"{api_out=}\")\n",
    "        return api_out[\"access_token\"]\n",
    "\n",
    "    options = ApiVlmOptions(\n",
    "        url=\"https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2023-05-29\",\n",
    "        params=dict(\n",
    "            model_id=model,\n",
    "            project_id=project_id,\n",
    "            parameters=dict(\n",
    "                max_new_tokens=400,\n",
    "            ),\n",
    "        ),\n",
    "        headers={\n",
    "            \"Authorization\": \"Bearer \" + _get_iam_access_token(api_key=api_key),\n",
    "        },\n",
    "        prompt=prompt,\n",
    "        timeout=60,\n",
    "        response_format=ResponseFormat.MARKDOWN,\n",
    "    )\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3463c6-1079-494b-bff5-a2bee6709d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "input_doc_path = Path(\"C:\\dev\\projects\\docling-experimentation\\data\\Vodafone 2025 Annual Report 10.pdf\")\n",
    "\n",
    "# Configure the VLM pipeline. Enabling remote services allows HTTP calls to\n",
    "# locally hosted APIs (LM Studio, Ollama) or cloud services.\n",
    "pipeline_options = VlmPipelineOptions(\n",
    "    enable_remote_services=True  # required when calling remote VLM endpoints\n",
    ")\n",
    "\n",
    "# The ApiVlmOptions() allows to interface with APIs supporting\n",
    "# the multi-modal chat interface. Here follow a few example on how to configure those.\n",
    "\n",
    "# One possibility is self-hosting the model, e.g., via LM Studio, Ollama or VLLM.\n",
    "#\n",
    "# e.g. with VLLM, serve granite-docling with these commands:\n",
    "# > vllm serve ibm-granite/granite-docling-258M --revision untied\n",
    "#\n",
    "# with LM Studio, serve granite-docling with these commands:\n",
    "# > lms server start\n",
    "# > lms load ibm-granite/granite-docling-258M-mlx\n",
    "\n",
    "# Example using the Granite-Docling model with LM Studio or VLLM:\n",
    "pipeline_options.vlm_options = openai_compatible_vlm_options(\n",
    "    model=\"zai-org/GLM-4.5V:novita\",  # For VLLM use \"ibm-granite/granite-docling-258M\"\n",
    "    hostname_and_port=\"router.huggingface.co\",  # LM studio defaults to port 1234, VLLM to 8000\n",
    "    prompt=\"Convert this page to Docling.\",\n",
    "    format=ResponseFormat.DOCTAGS,\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Example using the OlmOcr (dynamic prompt) model with LM Studio:\n",
    "# (uncomment the following lines)\n",
    "# pipeline_options.vlm_options = lms_olmocr_vlm_options(\n",
    "#     model=\"hf.co/lmstudio-community/olmOCR-7B-0225-preview-GGUF\",\n",
    "# )\n",
    "\n",
    "# Example using the Granite Vision model with Ollama:\n",
    "# (uncomment the following lines)\n",
    "# pipeline_options.vlm_options = ollama_vlm_options(\n",
    "#     model=\"granite3.2-vision:2b\",\n",
    "#     prompt=\"OCR the full page to markdown.\",\n",
    "# )\n",
    "\n",
    "# Another possibility is using online services, e.g., watsonx.ai.\n",
    "# Using watsonx.ai requires setting env variables WX_API_KEY and WX_PROJECT_ID\n",
    "# (see the top-level docstring for details). You can use a .env file as well.\n",
    "# (uncomment the following lines)\n",
    "# pipeline_options.vlm_options = watsonx_vlm_options(\n",
    "#     model=\"ibm/granite-vision-3-2-2b\", prompt=\"OCR the full page to markdown.\"\n",
    "# )\n",
    "\n",
    "# Create the DocumentConverter and launch the conversion.\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "            pipeline_cls=VlmPipeline,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "result = doc_converter.convert(source=input_doc_path)\n",
    "print(result.document.export_to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling-experimentation",
   "language": "python",
   "name": "docling-experimentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
